import os
import re
import logging
from urllib.parse import urlsplit, urljoin, unquote

import openpyxl
import pandas as pd
import requests
from bs4 import BeautifulSoup


class Parser:
    def __init__(self, session, main_url, save_path="./"):
        """–ö–æ–Ω—Å—Ç—Ä—É–∫—Ç–æ—Ä –ø–∞—Ä—Å–µ—Ä–∞."""
        self.session = session
        self.save_path = save_path
        self.main_url = main_url
        self.base_url = "https://ecu-firmware-files.ru"
        self.excel_path = os.path.join(self.save_path, "report.xlsx")
        if not os.path.exists(save_path):
            os.makedirs(save_path)

    def get_page_content(self, url):
        """–ó–∞–ø—Ä–∞—à–∏–≤–∞–µ—Ç HTML-–∫–æ–¥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫."""
        try:
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            return response.text
        except requests.exceptions.Timeout:
            logging.error(f"‚è≥ –¢–∞–π–º–∞—É—Ç –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {url}")
        except requests.exceptions.RequestException as e:
            logging.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–µ—Ç–∏ {url}: {e}")
        return None

    def find_thread_links(self, html):
        """–ù–∞—Ö–æ–¥–∏—Ç —Å—Å—ã–ª–∫–∏ –Ω–∞ —Ç–µ–º—ã."""
        soup = BeautifulSoup(html, "html.parser")
        return [
            self.base_url + a["href"].split("/unread")[0]
            for div in soup.find_all("div", class_="structItem-title")
            if (a := div.find("a", href=True))
        ]

    def extract_thread_id(self, thread_url):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç ID —Ç–µ–º—ã –∏–∑ URL."""
        match = re.search(r"/threads/([^/]+)(?:/|$)", thread_url)
        return match.group(1) if match else None

    def extract_articles(self, html):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å—Ç–∞—Ç—å–∏ –∏–∑ —Ç–µ–º—ã."""

        soup = BeautifulSoup(html, "html.parser")

        container = soup.find("div", class_="block-body js-replyNewMessageContainer")

        if not container:
            return []

        return container.find_all("article", class_="message")

    def parse_thread(self, thread_url):
        """–ü–∞—Ä—Å–∏—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—É —Ç–µ–º—ã –∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ."""
        html = self.get_page_content(thread_url)
        if not html:
            return None
        articles = self.extract_articles(html)

        titles, authors, texts = [], [], []

        for article in articles:
            author = article.find("a", class_="username")
            description = article.find("div", class_="bbCodeBlock-expandContent") or article.find("div",
                                                                                                  class_="bbWrapper")

            author = author.get_text(strip=True) if author else "–ê–≤—Ç–æ—Ä –Ω–µ –Ω–∞–π–¥–µ–Ω"
            text = description.get_text("\n", strip=True) if description else "–¢–µ–∫—Å—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω"

            authors.append(author)
            texts.append(text)

        # –°–±–æ—Ä —Å—Å—ã–ª–æ–∫ –Ω–∞ –≤–ª–æ–∂–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã

        soup = BeautifulSoup(html, "html.parser")
        attachments = [
            {"name": link["title"], "url": link["href"]}
            for link in soup.select("ul.attachmentList a[href]")
            if link.get("title")
        ]

        # –ü–æ–∏—Å–∫ –∫–Ω–æ–ø–∫–∏ "–°–∫–∞—á–∞—Ç—å"
        download_button = soup.select_one(".p-title-pageAction a.button--cta")
        if download_button and (download_url := download_button.get("href")):
            attachments.append({"name": "", "url": download_url})

        new_attachments = []
        for attachment in attachments:
            global_file_url = urljoin(self.base_url, attachment["url"])
            response = self.session.get(global_file_url, stream=True)
            response.raise_for_status()

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ Content-Type –∏ Content-Disposition –¥–ª—è —Ä–∞–∑–ª–∏—á–∏—è —Ñ–∞–π–ª–æ–≤ –∏ HTML-—Å—Ç—Ä–∞–Ω–∏—Ü
            content_type = response.headers.get("Content-Type", "")

            if "text/html" in content_type:
                file_info = self.get_file_info(global_file_url)
                for file in file_info:
                    new_attachments.append({"name": file["name"], "url": file["url"]})
            else:
                new_attachments.append({"name": attachment["name"], "url": attachment["url"]})

        attachments = new_attachments

        title_tag = soup.select_one(".p-title .p-title-value")
        title = title_tag.text.strip() if title_tag else ""

        dir_name = re.sub(r'[\\/|?&"<>* :]', '_', title)

        return {
            "title": title,
            "author": authors,
            "br_text": texts,
            "attachments": attachments,
            "dir_name": dir_name,
            "thread_url": thread_url,
        }

    def parse_forum(self):
        """–ü–∞—Ä—Å–∏—Ç –≤—Å–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Ñ–æ—Ä—É–º–∞ –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –¥–∞–Ω–Ω—ã–µ."""
        page_number = 1
        results = []
        forum_url = self.main_url
        while forum_url:
            html = self.get_page_content(forum_url)
            if not html:
                logging.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å {forum_url}.")
                break

            thread_links = self.find_thread_links(html)
            for thread_url in thread_links:
                if thread_data := self.parse_thread(thread_url):
                    results.append(thread_data)
                    self.save_thread_data(thread_data)

            # –ü–µ—Ä–µ—Ö–æ–¥ –Ω–∞ —Å–ª–µ–¥—É—é—â—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
            soup = BeautifulSoup(html, "html.parser")
            next_page = soup.select_one("a.pageNav-jump--next")
            forum_url = self.base_url + next_page["href"] if next_page else None
            page_number += 1

        print(f"–ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω. –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {page_number - 1} —Å—Ç—Ä–∞–Ω–∏—Ü, –Ω–∞–π–¥–µ–Ω–æ {len(results)} —Ç–µ–º.")
        return results

    def save_thread_data(self, data):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –¥–∞–Ω–Ω—ã–µ —Ç–µ–º—ã: —Ñ–∞–π–ª—ã, —Ç–µ–∫—Å—Ç, –æ—Ç—á–µ—Ç –≤ Excel."""
        thread_folder = os.path.join(self.save_path, data["dir_name"])
        os.makedirs(thread_folder, exist_ok=True)

        # 1Ô∏è‚É£ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ —Ñ–∞–π–ª–∞ —Å –æ–ø–∏—Å–∞–Ω–∏–µ–º
        self.save_text_file(data, thread_folder)

        # 2Ô∏è‚É£ –ü–æ–∏—Å–∫ –∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ –≤–ª–æ–∂–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
        self.download_attachments(data, thread_folder)

        # 3Ô∏è‚É£ –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –≤ `report.xlsx`
        self.update_report(data)

    def save_text_file(self, data, thread_folder):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª —Å –æ–ø–∏—Å–∞–Ω–∏–µ–º —Ç–µ–º—ã, –∏–∑–±–µ–≥–∞—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤."""
        txt_path = os.path.join(thread_folder, f"{data['dir_name']}.txt")

        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Å—Å—ã–ª–∫–∏ –Ω–∞ —Ç–µ–º—É (—É–±–∏—Ä–∞–µ–º –∑–∞–≤–µ—Ä—à–∞—é—â–∏–π —Å–ª–µ—à)
        normalized_url = data['thread_url'].rstrip('/')

        new_entries = []

        # –§–æ—Ä–º–∏—Ä—É–µ–º –Ω–æ–≤—ã–µ –∑–∞–ø–∏—Å–∏ –∏ –æ–±—ä–µ–¥–∏–Ω—è–µ–º –∏—Ö –≤ –µ–¥–∏–Ω—É—é —Å—Ç—Ä–æ–∫—É
        for author, text in zip(data["author"], data["br_text"]):
            normalized_text = text.strip()  # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –∏ –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏
            entry = f"–°—Å—ã–ª–∫–∞ –Ω–∞ —Ç–µ–º—É: {normalized_url}\n–ù–∞–∑–≤–∞–Ω–∏–µ: {data['title']}\n–ê–≤—Ç–æ—Ä: {author}\n–û–ø–∏—Å–∞–Ω–∏–µ:\n{normalized_text}\n"
            new_entries.append(entry)

        new_content = "".join(new_entries)  # –ü—Ä–µ–≤—Ä–∞—â–∞–µ–º –Ω–æ–≤—ã–µ –∑–∞–ø–∏—Å–∏ –≤ –µ–¥–∏–Ω—É—é —Å—Ç—Ä–æ–∫—É

        # –ß–∏—Ç–∞–µ–º –≤–µ—Å—å —Ñ–∞–π–ª –≤ —Å—Ç—Ä–æ–∫—É
        existing_content = ""
        if os.path.exists(txt_path):
            with open(txt_path, "r", encoding="utf-8") as f:
                existing_content = f.read()

        if new_content not in existing_content:
            with open(txt_path, "a", encoding="utf-8") as f:
                f.write(new_content)
                f.write("\n" * 5)
            print(f"‚úÖ –¢–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª —Å –æ–ø–∏—Å–∞–Ω–∏–µ–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {txt_path}")
        else:
            print(f"‚ö†Ô∏è –ó–∞–ø–∏—Å—å —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, —Ñ–∞–π–ª –ø—Ä–æ–ø—É—â–µ–Ω: {txt_path}")

    def download_attachments(self, data, thread_folder):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –≤—Å–µ –≤–ª–æ–∂–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã –≤ —Ç–µ–º—É."""
        for attachment in data["attachments"]:
            global_file_url = urljoin(self.base_url, attachment["url"])
            file_name = attachment["name"]

            try:
                if not self.check_file_url_exists(attachment["url"]):
                    self.download_file(global_file_url, thread_folder, file_name)
                else:
                    print(attachment["name"],"–ø—Ä–æ–ø—É—â–µ–Ω")
            except requests.RequestException as e:
                logging.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è {global_file_url}: {e}")

    def download_file(self, global_file_url, thread_folder, file_name=""):
        """–°–∫–∞—á–∏–≤–∞–µ—Ç —Ñ–∞–π–ª –ø–æ —Å—Å—ã–ª–∫–µ –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –µ–≥–æ –Ω–∞ –¥–∏—Å–∫."""

        response = self.session.get(global_file_url, stream=True)
        response.raise_for_status()

        # –ü–æ–ª—É—á–∞–µ–º –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–µ –∏–º—è —Ñ–∞–π–ª–∞
        file_name = self.get_filename_from_headers(response.headers, global_file_url, file_name)

        if file_name == "reply":
            print(f"‚ö†Ô∏è –ü—Ä–æ–ø—É—â–µ–Ω —Ñ–∞–π–ª —Å –∏–º–µ–Ω–µ–º 'reply': {global_file_url}")
            return

        file_path = os.path.join(thread_folder, file_name)
        file_path = self.ensure_unique_file_path(file_path)

        with open(file_path, "wb") as f:
            for chunk in response.iter_content(1024):
                f.write(chunk)

        print(f"‚úÖ –§–∞–π–ª {file_name} —Å–∫–∞—á–∞–Ω –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {file_path}")

    def get_filename_from_headers(self, headers, file_url, file_name):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∏–º—è —Ñ–∞–π–ª–∞ –∏–∑ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –∏–ª–∏ URL, –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—è –∫–∏—Ä–∏–ª–ª–∏—Ü—É."""
        content_disposition = headers.get("Content-Disposition")

        if not file_name:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º `filename*` —Å UTF-8 –∫–æ–¥–∏—Ä–æ–≤–∫–æ–π
            if content_disposition and "filename*" in content_disposition:
                file_name_encoded = content_disposition.split("filename*=")[1]
                encoding, file_name = file_name_encoded.split("''", 1)
                file_name = unquote(file_name)

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º `filename`
            elif content_disposition and "filename=" in content_disposition:
                file_name = content_disposition.split("filename=")[1].strip('"')

            # –ï—Å–ª–∏ –Ω–µ—Ç –∑–∞–≥–æ–ª–æ–≤–∫–∞, –±–µ—Ä–µ–º –∏–º—è –∏–∑ URL
            else:
                file_name = os.path.basename(urlsplit(file_url).path)
                file_name = unquote(file_name)  # –î–µ–∫–æ–¥–∏—Ä—É–µ–º –∫–∏—Ä–∏–ª–ª–∏—Ü—É

        return file_name

    def ensure_unique_file_path(self, file_path):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–∞–ª–∏—á–∏–µ —Ñ–∞–π–ª–∞ —Å —Ç–∞–∫–∏–º –∂–µ –∏–º–µ–Ω–µ–º –∏ –¥–æ–±–∞–≤–ª—è–µ—Ç –Ω–æ–º–µ—Ä, –µ—Å–ª–∏ —Ñ–∞–π–ª —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç."""
        counter = 1
        original_file_path = file_path
        while os.path.exists(file_path):
            name, ext = os.path.splitext(original_file_path)
            file_path = os.path.join(os.path.dirname(original_file_path), f"{name} - {counter}{ext}")
            counter += 1
        return file_path

    def check_file_url_exists(self, file_url):
        """–ú–µ—Ç–æ–¥ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ —Å—Å—ã–ª–∫–∞ –Ω–∞ —Ñ–∞–π–ª –≤ –æ—Ç—á–µ—Ç–µ."""
        if os.path.exists(self.excel_path):
            df = pd.read_excel(self.excel_path)
        else:
            df = pd.DataFrame(
                columns=["‚Ññ", "–°—Ç–∞—Ç—É—Å", "–ù–∞–∑–≤–∞–Ω–∏–µ —Ç–µ–º—ã", "–°—Å—ã–ª–∫–∞ –Ω–∞ —Ç–µ–º—É", "–°—Å—ã–ª–∫–∞ –Ω–∞ —Ñ–∞–π–ª", "–ù–∞–∑–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞"])

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ file_url –≤ –∫–æ–ª–æ–Ω–∫–µ "–°—Å—ã–ª–∫–∞ –Ω–∞ —Ñ–∞–π–ª"
        if file_url in df["–°—Å—ã–ª–∫–∞ –Ω–∞ —Ñ–∞–π–ª"].values:
            print(f"–°—Å—ã–ª–∫–∞ –Ω–∞ —Ñ–∞–π–ª {file_url} —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –≤ –æ—Ç—á–µ—Ç–µ.")
            return True
        else:
            return False

    def update_report(self, data):
        """–û–±–Ω–æ–≤–ª—è–µ—Ç –æ—Ç—á–µ—Ç –≤ Excel."""

        df = pd.read_excel(self.excel_path) if os.path.exists(self.excel_path) else pd.DataFrame(
            columns=["‚Ññ", "–°—Ç–∞—Ç—É—Å", "–ù–∞–∑–≤–∞–Ω–∏–µ —Ç–µ–º—ã", "–°—Å—ã–ª–∫–∞ –Ω–∞ —Ç–µ–º—É", "–°—Å—ã–ª–∫–∞ –Ω–∞ —Ñ–∞–π–ª", "–ù–∞–∑–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞"]
        )
        for attachment in data["attachments"]:
            if not self.check_file_url_exists(attachment["url"]):
                df = pd.concat([df, pd.DataFrame([{
                    "‚Ññ": len(df) + 1,
                    "–°—Ç–∞—Ç—É—Å": "–°–∫–∞—á–∞–Ω",
                    "–ù–∞–∑–≤–∞–Ω–∏–µ —Ç–µ–º—ã": data["title"],
                    "–°—Å—ã–ª–∫–∞ –Ω–∞ —Ç–µ–º—É": data["thread_url"],
                    "–°—Å—ã–ª–∫–∞ –Ω–∞ —Ñ–∞–π–ª": attachment["url"],
                    "–ù–∞–∑–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞": attachment["name"]
                }])], ignore_index=True)
                print(f"üìä –û—Ç—á–µ—Ç –æ–±–Ω–æ–≤–ª–µ–Ω: {self.excel_path}")

        df.to_excel(self.excel_path, index=False)

        self.check_and_add_base_url()

    def check_and_add_base_url(self):
        config_sheet_name = "config"

        if os.path.exists(self.excel_path):
            workbook = openpyxl.load_workbook(self.excel_path)
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ –ª–∏—Å—Ç config, –µ—Å–ª–∏ –Ω–µ—Ç - —Å–æ–∑–¥–∞–µ–º –µ–≥–æ
            if config_sheet_name not in workbook.sheetnames:
                sheet = workbook.create_sheet(config_sheet_name)
                # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –≥–ª–æ–±–∞–ª—å–Ω—ã–π URL –≤ –ø–µ—Ä–≤—É—é —è—á–µ–π–∫—É
                sheet.cell(row=1, column=1).value = self.main_url
                sheet.sheet_state = "hidden"  # –°–∫—Ä—ã–≤–∞–µ–º –ª–∏—Å—Ç
            else:
                sheet = workbook[config_sheet_name]
                # –ï—Å–ª–∏ URL –Ω–µ –∑–∞–ø–∏—Å–∞–Ω, –¥–æ–±–∞–≤–ª—è–µ–º –µ–≥–æ
                if not sheet.cell(row=1, column=1).value:
                    sheet.cell(row=1, column=1).value = self.main_url
                    sheet.sheet_state = "hidden"  # –°–∫—Ä—ã–≤–∞–µ–º –ª–∏—Å—Ç

            workbook.save(self.excel_path)
        else:
            print("–§–∞–π–ª –æ—Ç—á–µ—Ç–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω!")

    def get_file_info(self, page_url):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∏–º—è —Ñ–∞–π–ª–∞ –∏ —Å—Å—ã–ª–∫—É –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è —Å HTML —Å—Ç—Ä–∞–Ω–∏—Ü—ã."""
        try:
            response = self.session.get(page_url)
            response.raise_for_status()

            soup = BeautifulSoup(response.text, "html.parser")
            file_info = []

            file_items = soup.select(".block-body .block-row")

            for item in file_items:
                file_name_tag = item.select_one(".contentRow-title")
                file_link_tag = item.select_one(".contentRow-extra a")

                if file_name_tag and file_link_tag:
                    file_name = file_name_tag.text.strip()
                    file_url = urljoin(self.base_url, file_link_tag["href"])

                    file_info.append({"name": file_name, "url": file_url})

            return file_info

        except requests.RequestException as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {e}")
            return []
