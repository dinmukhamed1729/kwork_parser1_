import os
import re
import logging
from urllib.parse import urlsplit, urljoin, unquote

import pandas as pd
import requests
from bs4 import BeautifulSoup


class Parser:
    def __init__(self, session, save_path="./"):
        """–ö–æ–Ω—Å—Ç—Ä—É–∫—Ç–æ—Ä –ø–∞—Ä—Å–µ—Ä–∞."""
        self.session = session
        self.save_path = save_path
        self.base_url = "https://ecu-firmware-files.ru"
        self.title = None
        if not os.path.exists(save_path):
            os.makedirs(save_path)

    def get_page_content(self, url):
        """–ó–∞–ø—Ä–∞—à–∏–≤–∞–µ—Ç HTML-–∫–æ–¥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫."""
        try:
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            return response.text
        except requests.exceptions.Timeout:
            logging.error(f"‚è≥ –¢–∞–π–º–∞—É—Ç –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {url}")
        except requests.exceptions.RequestException as e:
            logging.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–µ—Ç–∏ {url}: {e}")
        return None

    def find_thread_links(self, html):
        """–ù–∞—Ö–æ–¥–∏—Ç —Å—Å—ã–ª–∫–∏ –Ω–∞ —Ç–µ–º—ã."""
        soup = BeautifulSoup(html, "html.parser")
        return [
            self.base_url + a["href"].split("/unread")[0]
            for div in soup.find_all("div", class_="structItem-title")
            if (a := div.find("a", href=True))
        ]

    def extract_thread_id(self, thread_url):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç ID —Ç–µ–º—ã –∏–∑ URL."""
        match = re.search(r"/threads/([^/]+)(?:/|$)", thread_url)
        return match.group(1) if match else None

    def extract_articles(self, html):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å—Ç–∞—Ç—å–∏ –∏–∑ —Ç–µ–º—ã."""


        soup = BeautifulSoup(html, "html.parser")
        self.title = soup.find("h1", class_="p-title-value")
        container = soup.find("div", class_="block-body js-replyNewMessageContainer")

        if not container:
            return []

        return container.find_all("article", class_="message")

    def parse_thread(self, thread_url):
        """–ü–∞—Ä—Å–∏—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—É —Ç–µ–º—ã –∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ."""
        html = self.get_page_content(thread_url)
        if not html:
            return None
        articles = self.extract_articles(html)

        titles, authors, texts = [], [], []

        for article in articles:

            author = article.find("a", class_="username")
            description = article.find("div", class_="bbCodeBlock-expandContent") or article.find("div", class_="bbWrapper")


            author = author.get_text(strip=True) if author else "–ê–≤—Ç–æ—Ä –Ω–µ –Ω–∞–π–¥–µ–Ω"
            text = description.get_text("\n", strip=True) if description else "–¢–µ–∫—Å—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω"

            authors.append(author)
            texts.append(text)

        title = self.title.get_text(strip=True) if self.title else "–ó–∞–≥–æ–ª–æ–≤–æ–∫ –Ω–µ –Ω–∞–π–¥–µ–Ω"
        # –°–±–æ—Ä —Å—Å—ã–ª–æ–∫ –Ω–∞ –≤–ª–æ–∂–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã

        soup = BeautifulSoup(html, "html.parser")
        attachments = [
            {"name": link["title"], "url": link["href"]}
            for link in soup.select("ul.attachmentList a[href]")
            if link.get("title")
        ]

        # –ü–æ–∏—Å–∫ –∫–Ω–æ–ø–∫–∏ "–°–∫–∞—á–∞—Ç—å"
        download_button = soup.select_one(".p-title-pageAction a.button--cta")
        if download_button and (download_url := download_button.get("href")):
            attachments.append({"name": "", "url": download_url})

        title_tag = soup.select_one(".p-title .p-title-value")
        dir_name = title_tag.text.strip() if title_tag else ""

        return {
            "title": title,
            "author": authors,
            "br_text": texts,
            "attachments": attachments,
            "dir_name": dir_name,
            "thread_url": thread_url,
        }

    def parse_forum(self, forum_url):
        """–ü–∞—Ä—Å–∏—Ç –≤—Å–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Ñ–æ—Ä—É–º–∞ –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –¥–∞–Ω–Ω—ã–µ."""
        page_number = 1
        results = []

        while forum_url:
            html = self.get_page_content(forum_url)
            if not html:
                logging.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å {forum_url}.")
                break

            thread_links = self.find_thread_links(html)
            for thread_url in thread_links:
                if thread_data := self.parse_thread(thread_url):
                    results.append(thread_data)
                    self.save_thread_data(thread_data)

            # –ü–µ—Ä–µ—Ö–æ–¥ –Ω–∞ —Å–ª–µ–¥—É—é—â—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
            soup = BeautifulSoup(html, "html.parser")
            next_page = soup.select_one("a.pageNav-jump--next")
            forum_url = self.base_url + next_page["href"] if next_page else None
            page_number += 1

        print(f"–ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω. –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {page_number - 1} —Å—Ç—Ä–∞–Ω–∏—Ü, –Ω–∞–π–¥–µ–Ω–æ {len(results)} —Ç–µ–º.")
        return results

    def save_thread_data(self, data):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –¥–∞–Ω–Ω—ã–µ —Ç–µ–º—ã: —Ñ–∞–π–ª—ã, —Ç–µ–∫—Å—Ç, –æ—Ç—á–µ—Ç –≤ Excel."""
        thread_folder = os.path.join(self.save_path, data["dir_name"])
        os.makedirs(thread_folder, exist_ok=True)

        # 1Ô∏è‚É£ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ —Ñ–∞–π–ª–∞ —Å –æ–ø–∏—Å–∞–Ω–∏–µ–º
        self.save_text_file(data, thread_folder)

        # 2Ô∏è‚É£ –ü–æ–∏—Å–∫ –∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ –≤–ª–æ–∂–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
        self.download_attachments(data, thread_folder)

        # 3Ô∏è‚É£ –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –≤ `report.xlsx`
        self.update_report(data)

    def save_text_file(self, data, thread_folder):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª —Å –æ–ø–∏—Å–∞–Ω–∏–µ–º —Ç–µ–º—ã."""
        txt_path = os.path.join(thread_folder, f"{data['dir_name']}.txt")
        with open(txt_path, "a", encoding="utf-8") as f:
            f.write(f"–°—Å—ã–ª–∫–∞ –Ω–∞ —Ç–µ–º—É: {data['thread_url']}\n")
            for author, text in zip(data["author"], data["br_text"]):
                f.write(f"–ù–∞–∑–≤–∞–Ω–∏–µ: {data['title']}\n")
                f.write(f"–ê–≤—Ç–æ—Ä: {author}\n")
                f.write(f"–û–ø–∏—Å–∞–Ω–∏–µ:\n{text}\n")
                f.write("\n" * 5)

        print(f"‚úÖ –¢–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª —Å –æ–ø–∏—Å–∞–Ω–∏–µ–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {txt_path}")

    def download_attachments(self, data, thread_folder):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –≤—Å–µ –≤–ª–æ–∂–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã –≤ —Ç–µ–º—É."""
        for attachment in data["attachments"]:
            file_url = urljoin(self.base_url, attachment["url"])
            file_name = attachment["name"]

            try:
                self.download_file(file_url, thread_folder, file_name)
            except requests.RequestException as e:
                logging.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è {file_url}: {e}")

    def download_file(self, file_url, thread_folder, file_name=""):
        """–°–∫–∞—á–∏–≤–∞–µ—Ç —Ñ–∞–π–ª –ø–æ —Å—Å—ã–ª–∫–µ –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –µ–≥–æ –Ω–∞ –¥–∏—Å–∫."""
        response = self.session.get(file_url, stream=True)
        response.raise_for_status()

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ Content-Type –∏ Content-Disposition –¥–ª—è —Ä–∞–∑–ª–∏—á–∏—è —Ñ–∞–π–ª–æ–≤ –∏ HTML-—Å—Ç—Ä–∞–Ω–∏—Ü
        content_type = response.headers.get("Content-Type", "")

        if "text/html" in content_type:
            print("+++++++++++++++++++++++++++++++")
            file_info = self.get_file_info(file_url)
            print(file_info)
            for file in file_info:
                self.download_file(file["file_url"], thread_folder, file["file_name"])
            return

        # –ü–æ–ª—É—á–∞–µ–º –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–µ –∏–º—è —Ñ–∞–π–ª–∞
        file_name = self.get_filename_from_headers(response.headers, file_url, file_name)

        if file_name == "reply":
            print(f"‚ö†Ô∏è –ü—Ä–æ–ø—É—â–µ–Ω —Ñ–∞–π–ª —Å –∏–º–µ–Ω–µ–º 'reply': {file_url}")
            return

        file_path = os.path.join(thread_folder, file_name)
        file_path = self.ensure_unique_file_path(file_path)

        with open(file_path, "wb") as f:
            for chunk in response.iter_content(1024):
                f.write(chunk)

        print(f"‚úÖ –§–∞–π–ª {file_name} —Å–∫–∞—á–∞–Ω –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {file_path}")

    def get_filename_from_headers(self, headers, file_url, file_name):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∏–º—è —Ñ–∞–π–ª–∞ –∏–∑ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –∏–ª–∏ URL, –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—è –∫–∏—Ä–∏–ª–ª–∏—Ü—É."""
        content_disposition = headers.get("Content-Disposition")

        if not file_name:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º `filename*` —Å UTF-8 –∫–æ–¥–∏—Ä–æ–≤–∫–æ–π
            if content_disposition and "filename*" in content_disposition:
                file_name_encoded = content_disposition.split("filename*=")[1]
                encoding, file_name = file_name_encoded.split("''", 1)
                file_name = unquote(file_name)

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º `filename`
            elif content_disposition and "filename=" in content_disposition:
                file_name = content_disposition.split("filename=")[1].strip('"')

            # –ï—Å–ª–∏ –Ω–µ—Ç –∑–∞–≥–æ–ª–æ–≤–∫–∞, –±–µ—Ä–µ–º –∏–º—è –∏–∑ URL
            else:
                file_name = os.path.basename(urlsplit(file_url).path)
                file_name = unquote(file_name)  # –î–µ–∫–æ–¥–∏—Ä—É–µ–º –∫–∏—Ä–∏–ª–ª–∏—Ü—É

        return file_name

    def ensure_unique_file_path(self, file_path):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–∞–ª–∏—á–∏–µ —Ñ–∞–π–ª–∞ —Å —Ç–∞–∫–∏–º –∂–µ –∏–º–µ–Ω–µ–º –∏ –¥–æ–±–∞–≤–ª—è–µ—Ç –Ω–æ–º–µ—Ä, –µ—Å–ª–∏ —Ñ–∞–π–ª —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç."""
        counter = 1
        original_file_path = file_path
        while os.path.exists(file_path):
            name, ext = os.path.splitext(original_file_path)
            file_path = os.path.join(os.path.dirname(original_file_path), f"{name} - {counter}{ext}")
            counter += 1
        return file_path

    def update_report(self, data):
        """–û–±–Ω–æ–≤–ª—è–µ—Ç –æ—Ç—á–µ—Ç –≤ Excel."""
        excel_path = os.path.join(self.save_path, "report.xlsx")

        df = pd.read_excel(excel_path) if os.path.exists(excel_path) else pd.DataFrame(
            columns=["‚Ññ", "–°—Ç–∞—Ç—É—Å", "–ù–∞–∑–≤–∞–Ω–∏–µ —Ç–µ–º—ã", "–°—Å—ã–ª–∫–∞ –Ω–∞ —Ç–µ–º—É"]
        )

        if data["thread_url"] not in df["–°—Å—ã–ª–∫–∞ –Ω–∞ —Ç–µ–º—É"].values:
            df = pd.concat([df, pd.DataFrame([{
                "‚Ññ": len(df) + 1,
                "–°—Ç–∞—Ç—É—Å": "–°–∫–∞—á–∞–Ω",
                "–ù–∞–∑–≤–∞–Ω–∏–µ —Ç–µ–º—ã": data["title"],
                "–°—Å—ã–ª–∫–∞ –Ω–∞ —Ç–µ–º—É": data["thread_url"]
            }])], ignore_index=True)

        df.to_excel(excel_path, index=False)
        print(f"üìä –û—Ç—á–µ—Ç –æ–±–Ω–æ–≤–ª–µ–Ω: {excel_path}")

    def get_file_info(self, page_url):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∏–º—è —Ñ–∞–π–ª–∞ –∏ —Å—Å—ã–ª–∫—É –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è —Å HTML —Å—Ç—Ä–∞–Ω–∏—Ü—ã."""
        try:
            response = self.session.get(page_url)
            response.raise_for_status()

            soup = BeautifulSoup(response.text, "html.parser")
            file_info = []

            file_items = soup.select(".block-body .block-row")

            for item in file_items:
                file_name_tag = item.select_one(".contentRow-title")
                file_link_tag = item.select_one(".contentRow-extra a")

                if file_name_tag and file_link_tag:
                    file_name = file_name_tag.text.strip()
                    file_url = urljoin(self.base_url, file_link_tag["href"])

                    file_info.append({"file_name": file_name, "file_url": file_url})

            return file_info

        except requests.RequestException as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {e}")
            return []

#std_28097344_9AWAEU41_combiloader.bin